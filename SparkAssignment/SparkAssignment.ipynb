{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Spark\n",
    "Some meme magic to be able to use Spark locally. If this does not work for you, just replace all of this with whatever you use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/apache-spark\n",
      "$SPARK_HOME/python:$SPARK_HOME/python/build\n"
     ]
    }
   ],
   "source": [
    "# Print some env variables\n",
    "# Commented-out lines were old tests to set up the environment variables from\n",
    "# Python, but this turned out to either not work or not be needed anyway.\n",
    "import os\n",
    "#os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
    "#os.environ[\"SPARK_HOME\"] = \"/opt/apache-spark\"\n",
    "#os.environ[\"PYTHONPATH\"] = os.path.join(os.environ[\"SPARK_HOME\"], \"python\") + os.pathsep + os.path.join(os.environ[\"SPARK_HOME\"], \"python/build\")\n",
    "#os.environ[\"PATH\"] += os.pathsep + os.path.join(os.environ[\"SPARK_HOME\"], \"python\") + os.pathsep + os.path.join(os.environ[\"SPARK_HOME\"], \"python/build\")\n",
    "print(os.environ[\"SPARK_HOME\"]) # /opt/apache-spark\n",
    "print(os.environ[\"PYTHONPATH\"]) # $SPARK_HOME/python:$SPARK_HOME/python/build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was the most convenient way to make Spark work for me\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# WARNING: You may want to customise the following values to your setup\n",
    "USE_FULL_CLUSTER = False\n",
    "if USE_FULL_CLUSTER:\n",
    "    # This line is for the 3-node hardware cluster that we set up in class for the memes:\n",
    "    #   - osmium (Librem Mini v1, Intel Core i7-8565U (4c/8t), 32 GiB DDR4) - MASTER\n",
    "    #   - graphite (HP ProBook 4740s, Intel Core i7-2670QM (4c/8t), 8 GiB DDR3)\n",
    "    #   - bloodfest (Prodrive Hermes, Intel Xeon E-2226G (6c/6t), 32 GiB DDR4)\n",
    "    conf = SparkConf().setMaster(\"spark://osmium:7077\").setAppName(\"My ASSignment\")\n",
    "else:\n",
    "    # This line is for the \"cluster\" with a single master/worker node (graphite), which\n",
    "    # is optimised for portability. No need to carry and set up a mini PC, a full ATX\n",
    "    # server/workstation mainboard on a cardboard box and a router EVERY time; just plop\n",
    "    # down the laptop, start the Spark services and you're ready to ignite the Spark :^)\n",
    "    conf = SparkConf().setMaster(\"spark://127.0.0.1:7077\").setAppName(\"My ASSignment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "J0GsQcjjNOep"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/28 17:23:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "#sc.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual Assignment Begins Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "G = nx.DiGraph()\n",
    "G.add_weighted_edges_from([\n",
    "    (\"A\", \"B\", 3.0), (\"A\", \"C\", 10.0), (\"A\", \"E\", 4.0),\n",
    "    (\"B\", \"C\", 2.0), (\"B\", \"D\", 8.0),  (\"B\", \"F\", 7.0),\n",
    "    (\"C\", \"D\", 5.0), (\"C\", \"G\", 3.0),\n",
    "    (\"D\", \"H\", 6.0),\n",
    "    (\"E\", \"F\", 2.0), (\"E\", \"I\", 9.0),\n",
    "    (\"F\", \"G\", 1.0), (\"F\", \"J\", 5.0),\n",
    "    (\"G\", \"H\", 2.0), (\"G\", \"K\", 4.0),\n",
    "    (\"I\", \"J\", 3.0),\n",
    "    (\"J\", \"K\", 6.0),\n",
    "])\n",
    "\n",
    "# or https://networkx.org/documentation/stable/reference/generators.html\n",
    "# G = nx.barabasi_albert_graph(100, 3)\n",
    "# G = nx.watts_strogatz_graph(100, 4, 0.1)\n",
    "# G = nx.random_geometric_graph(100, 0.1)\n",
    "# G = nx.random_k_out_graph(100, 4, 0.5)\n",
    "# G = nx.random_k_graph(100, 4)\n",
    "# G = nx.random_power_law_graph(100, 3, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class GraphNode:\n",
    "    def __init__(self, name, neighbours):\n",
    "        self.name = name\n",
    "        self.neighbours = neighbours\n",
    "        self.distance = math.inf\n",
    "        self.visited = False\n",
    "        self.path = []\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"('{self.name}', ({self.neighbours}, {self.distance}, {self.visited}, {self.path}))\"\n",
    "\n",
    "def to_node_graph(nx_graph, start_city):\n",
    "    graph = {node: GraphNode(node, {nbr: nx_graph.edges[node, nbr][\"weight\"] for nbr in nx_graph.successors(node)}) for node in nx_graph.nodes()}\n",
    "    start_node = graph.get(start_city)\n",
    "    if start_node is None:\n",
    "        raise KeyError(f\"City '{start_city}' not in graph\")\n",
    "    start_node.distance = 0\n",
    "    return graph\n",
    "\n",
    "def make_node_graph():\n",
    "    return to_node_graph(G, \"A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: ('A', ({'B': 3.0, 'C': 10.0, 'E': 4.0}, 0, False, []))\n",
      "B: ('B', ({'C': 2.0, 'D': 8.0, 'F': 7.0}, inf, False, []))\n",
      "C: ('C', ({'D': 5.0, 'G': 3.0}, inf, False, []))\n",
      "E: ('E', ({'F': 2.0, 'I': 9.0}, inf, False, []))\n",
      "D: ('D', ({'H': 6.0}, inf, False, []))\n",
      "F: ('F', ({'G': 1.0, 'J': 5.0}, inf, False, []))\n",
      "G: ('G', ({'H': 2.0, 'K': 4.0}, inf, False, []))\n",
      "H: ('H', ({}, inf, False, []))\n",
      "I: ('I', ({'J': 3.0}, inf, False, []))\n",
      "J: ('J', ({'K': 6.0}, inf, False, []))\n",
      "K: ('K', ({}, inf, False, []))\n"
     ]
    }
   ],
   "source": [
    "for city, node in make_node_graph().items():\n",
    "    print(f\"{city}: {node}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Implementation\n",
    "Done to use as reference to verify the parallel implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_solver(graph):\n",
    "    while True:\n",
    "        closest_node = None\n",
    "        for node in graph.values():\n",
    "            if not node.visited and (closest_node is None or node.distance < closest_node.distance):\n",
    "                closest_node = node\n",
    "\n",
    "        if closest_node is None:\n",
    "            break\n",
    "\n",
    "        closest_node.visited = True\n",
    "        # Original path was a reference to another path, make a copy before modifying.\n",
    "        # This is done to avoid making unnecessary list copies inside the loop below.\n",
    "        closest_node.path = closest_node.path.copy()\n",
    "        closest_node.path.append(closest_node.name)\n",
    "        for nbr, nbr_dist in closest_node.neighbours.items():\n",
    "            node = graph.get(nbr)\n",
    "            if node is None or node.visited:\n",
    "                continue\n",
    "\n",
    "            new_dist = closest_node.distance + nbr_dist\n",
    "            if new_dist < node.distance:\n",
    "                node.distance = new_dist\n",
    "                # Delay making copies until we need to modify this.\n",
    "                node.path = closest_node.path\n",
    "    return sorted(graph.values(), key=lambda node: node.name)\n",
    "\n",
    "seq_node_graph = sequential_solver(make_node_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "def parallel_solver(in_graph):\n",
    "    def one_iteration(graph):\n",
    "        # We absolutely have to make a new RDD each iteration, or else we end up chaining\n",
    "        # operations across iterations for some reason. Which is HORRIBLE for performance.\n",
    "        rdd = sc.parallelize(graph)\n",
    "        def fold_nodes(u, v):\n",
    "            if u is None:\n",
    "                return v\n",
    "            elif v is None:\n",
    "                return u\n",
    "            else:\n",
    "                return v if u.distance > v.distance else u \n",
    "        closest_node = rdd.filter(lambda node: not node.visited).fold(None, fold_nodes)\n",
    "        if closest_node is None:\n",
    "            return rdd.collect(), True\n",
    "\n",
    "        closest_node.visited = True\n",
    "        # Original path was a reference to another path, make a copy before modifying.\n",
    "        # This is done to avoid making unnecessary list copies inside the loop below.\n",
    "        closest_node.path = closest_node.path.copy()\n",
    "        closest_node.path.append(closest_node.name)\n",
    "        def update_node(t):\n",
    "            (node, nbr_dist) = t\n",
    "            node.distance = closest_node.distance + nbr_dist\n",
    "            # Delay making copies until we need to modify this.\n",
    "            node.path = closest_node.path\n",
    "            return node\n",
    "        # Since RDDs are immutable, we have to create a new RDD with the updated values\n",
    "        # Behold: this (un)holy line of code is functional programming at its peak\n",
    "        upd = rdd.filter(lambda node: not node.visited).map(lambda node: (node, closest_node.neighbours.get(node.name))).filter(lambda t: t[1] is not None).filter(lambda t: closest_node.distance + t[1] < t[0].distance).map(update_node)\n",
    "        # Now we have to convert the RDDs to a key-value pair to do a left outer join. We\n",
    "        # also have to update the closest node, thankfully we can throw it in with a union.\n",
    "        def to_kv(node):\n",
    "            return (node.name, node)\n",
    "        def to_node(kv):\n",
    "            return kv[1]\n",
    "        def merge_kvrdd(kvrdd_base, kvrdd_over):\n",
    "            def coalesce_joined_kvrdd(kv2):\n",
    "                (k, (lv, rv)) = kv2\n",
    "                return (k, lv) if rv is None else (k, rv)\n",
    "            return kvrdd_base.leftOuterJoin(kvrdd_over).map(coalesce_joined_kvrdd)\n",
    "        new_rdd = merge_kvrdd(rdd.map(to_kv), upd.map(to_kv).union(sc.parallelize([to_kv(closest_node)]))).map(to_node)\n",
    "        return new_rdd.collect(), False\n",
    "\n",
    "    graph = in_graph.values()\n",
    "    for _ in range(len(graph)):\n",
    "        graph, do_exit = one_iteration(graph)\n",
    "        if do_exit:\n",
    "            break\n",
    "    # Sorted so that comparisons are easy to make\n",
    "    graph.sort(key=lambda node: node.name)\n",
    "    return graph\n",
    "\n",
    "par_node_graph = parallel_solver(make_node_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Comparison\n",
    "They should be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq:\n",
      "A: 0, True, ['A']\n",
      "B: 3.0, True, ['A', 'B']\n",
      "C: 5.0, True, ['A', 'B', 'C']\n",
      "D: 10.0, True, ['A', 'B', 'C', 'D']\n",
      "E: 4.0, True, ['A', 'E']\n",
      "F: 6.0, True, ['A', 'E', 'F']\n",
      "G: 7.0, True, ['A', 'E', 'F', 'G']\n",
      "H: 9.0, True, ['A', 'E', 'F', 'G', 'H']\n",
      "I: 13.0, True, ['A', 'E', 'I']\n",
      "J: 11.0, True, ['A', 'E', 'F', 'J']\n",
      "K: 11.0, True, ['A', 'E', 'F', 'G', 'K']\n",
      "Par:\n",
      "A: 0, True, ['A']\n",
      "B: 3.0, True, ['A', 'B']\n",
      "C: 5.0, True, ['A', 'B', 'C']\n",
      "D: 10.0, True, ['A', 'B', 'C', 'D']\n",
      "E: 4.0, True, ['A', 'E']\n",
      "F: 6.0, True, ['A', 'E', 'F']\n",
      "G: 7.0, True, ['A', 'E', 'F', 'G']\n",
      "H: 9.0, True, ['A', 'E', 'F', 'G', 'H']\n",
      "I: 13.0, True, ['A', 'E', 'I']\n",
      "J: 11.0, True, ['A', 'E', 'F', 'J']\n",
      "K: 11.0, True, ['A', 'E', 'F', 'G', 'K']\n"
     ]
    }
   ],
   "source": [
    "print(\"Seq:\")\n",
    "for node in seq_node_graph:\n",
    "    print(f\"{node.name}: {node.distance}, {node.visited}, {node.path}\")\n",
    "print(\"Par:\")\n",
    "for node in par_node_graph:\n",
    "    print(f\"{node.name}: {node.distance}, {node.visited}, {node.path}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
